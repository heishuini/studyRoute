## ***<u>一、线性回归</u>***

### ***1.1 单变量线性回归***

$$
h_\theta = \theta_0+\theta_1x
$$

为了求出参数，目标是求出使得**建模误差**(模型预测值与训练集实际值之间的差距)的平方和最小的模型参数。

平方误差代价函数：
$$
\min_{\theta_0,\theta_1} J(\theta_0,\theta_1)=\min_{\theta_0,\theta_1}\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2
$$
求解该函数的算法：梯度下降（迭代算法，适合数据量大），正规方程法（normal equations 直接求数值解）

**· 梯度下降法**

![image-20240404192915478](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240404192915478.png)

开始时，随机选择一个参数组合()，寻找下一个使得代价函数值下降最多的参数组合，直至遇到局部最小值。
$$
\theta_j:=\theta_j-\alpha \frac{ \partial J(\theta_0,\theta_1) }{ \partial \theta_j }，(j = 0或1)
$$
α是学习率，求导相当于是该点的斜率

·α太小，很多步才能到达最低点

·α太大，可能越过最低点

·假设将θ放到局部最低点，那么下一步是不变，因为求导为0.  **故可解释为什么该算法可以收敛。**

批量梯度下降：梯度下降的每一步中，都用到了所有的训练样本，因为求导的时候，是求和



### ***1.2 线性代数知识回顾***

·矩阵和向量   向量是特殊的矩阵，一般指列向量

·矩阵乘法   不可交换律，可结合律

·单位矩阵I 主对角线上是1，其余全是0  

·逆，转置 
$$
AA^{-1}=I \\
（A×B）^T=B^T×A^T
$$

### ***1.3 多变量线性回归***

一般把Θ和X这种，都当成是列向量，所以Θ要转置。
$$
h_\theta(x)=\theta_0x_0+\theta_1x_1+...+\theta_nx_n = \theta^TX\\
\min_{\theta} J(\theta)=\min_{\theta}\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2\\
\theta_j:=\theta_j-\alpha \frac{ \partial J(\theta) }{ \partial \theta_j }，(j = 0,1,..,n)\\
↓\\
\theta_j:=\theta_j-\alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}，(j = 0,1,..,n)
$$

```python
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```

·特征缩放

​    为了帮助梯度下降法更快收敛，需要对特征的值进行缩放。
​    如特征1的值是0~2000，特征2的值是0~5，那么可以以二者为x，y轴画一个J(θ)的等高线，会发现图像很扁，收敛较慢

![image-20240404201001203](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240404201001203.png)

​    故需要将特征值缩到-1~1之间，
$$
x_n=\frac{x_n-\overline{x}}{s_n},s_n为标准差
$$
·学习率

   可绘制损失函数与迭代次数的图表查看收敛；
   或者设定函数值与阈值差距
   通常考虑设定α为0.01 0.03 0.1 0.3 1 3 10



### ***1.4 特征与多项式回归***

根据数据情况判断
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2\sqrt{x_3}
$$

### ***1.5 正规方程法***

对于某些线性回归问题，要求：矩阵可逆

**矩阵不可逆**通常是指 那些特征之间并不独立，或者 特征数量>>样本数量
$$
J(\theta)=a\theta^2+b\theta+c\\
求\frac{\partial J(\theta_j)}{\partial \theta_j} = 0\\
得\theta=(X^TX)^{-1}X^Ty\\
推导过程：(X是m行n列，\theta是n行1列)\\
J(\theta)=\min_{\theta}\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2\\
J(\theta)=\frac{1}{2}(X\theta-y)^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)\\
=\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta-y^Ty)
$$

![image-20240404205049705](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240404205049705.png)

| 梯度下降                   | 正规方程                                                     |
| :------------------------- | :----------------------------------------------------------- |
| 需要选择学习率             | 不需要                                                       |
| 需要多次迭代               | 一次运算得出                                                 |
| 当特征数量大时也能较好适用 | 需要计算$$（X^T X）^{-1}$$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O($$n^3$$)，通常来说当小于10000 时还是可以接受的 |
| 适用于各种类型的模型       | 只适用于线性模型，不适合逻辑回归模型等其他模型               |

```python
import numpy as np
 def normalEqn(X, y):
   theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X等价于X.T.dot(X)
   return theta
```

pinv即使矩阵不可逆也能计算(伪逆)

在发现矩阵不可逆时：

·寻找特征之间相关性，考虑只保留重复特征中的一个

·正则化技术：即使小数据样本，也能找多特征的参数

### ***1.6 code***

单变量线性回归：

​    查看数据样子->代价函数计算，输入矩阵X，y，theta，查看函数值

​	      ->批量梯度下降法，输入矩阵X,y,theta,alpha,迭代次数 -> 绘制图形，查看拟合情况； 绘制迭代图

多变量线性回归：

​    预处理步骤：特征归一化 -> 批量梯度下降法 同上

​		        -> scikt_learn的线性回归函数

​                       -> 正规方程法

## ***<u>二、逻辑回归</u>***

### ***2.1 定义***

输出值永远是0~1之间，
$$
令h_\theta(x)=sigmoid(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\\
h_\theta(x)>=0.5, y=1\\
h_\theta(x)<0.5, y=0\\
即h_\theta(x)=P(y=1|x;\theta)，在x和\theta的作用下，y=1的概率
$$

```python
import numpy as np
def sigmoid(z):
   return 1 / (1 + np.exp(-z))
```

![image-20240405141144459](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240405141144459.png)

### ***2.2 分类边界*** 

绘制  $$\theta^Tx=0$$ 的图像，但仅适合线性的数据
$$
由sigmoid图像可知，\\
\theta^Tx>0，h_\theta(x)>0.5，则y=1\\
\theta^Tx<0，h_\theta(x)<0.5，则y=0
$$

### ***2.3 代价函数***

如何确定$$\theta$$的取值？ 

如果还是采用误差的平方和，带入到函数中，得到的是一个非凸函数，**即在梯度下降算法中，很容易局部最小值。**

![image-20240405142057877](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240405142057877.png)

·引入对数方法：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})\\
Cost(h_\theta(x),y)=-log(h_\theta(x))，if\ y=1\\
=-log(1-h_\theta(x)),if\ y=0\\
即Cost(h_\theta(x^{(i)}),y^{(i)})=-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))
$$
当y=1，且$$h_\theta(x)=1$$，则误差为0

当y=1，且$$h_\theta(x)\ne1$$，随着$$h_\theta(x)$$的变小，误差会变大

当y=0，且$$h_\theta(x)=0$$，则误差为0

当y=0，且$$h_\theta(x)\ne0$$，随着$$h_\theta(x)$$的变大，误差会变大

```python
import numpy as np
def cost(theta, X, y):
  theta = np.matrix(theta)
  X = np.matrix(X)
  y = np.matrix(y)
  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
  second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
  return np.sum(first - second) / (len(X))
```

·梯度下降法求解，要先进行特征缩放(即归一化)
$$
\theta_j:=\theta_j-\alpha \frac{ \partial J(\theta) }{ \partial \theta_j }=\theta_j-\alpha \frac{1
}{m}\sum_{i=1}^m[h_\theta(x^{(i)})-y^{(i)})]x_j^{(i)}
$$
对于该凸函数的求解，考虑似然函数。 **发现该式子和线性回归的代价函数一样**

![image-20240405143506504](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240405143506504.png)

### ***2.4 高级优化***

​    提高逻辑回归的分类速度，解决大型机器学习问题

无需设置学习率$$\alpha$$

·共轭梯度BFGS（变尺度法）



·L-BFGS（限制变尺度法）

### ***2.5 多分类***

每个类别单独做二分类，然后选择该样本的$$h_\theta(x)$$最高的那个类别

### ***2.6 正则化***

**过拟合问题**：代价函数几乎为0，训练集训练得好，但是预测新数据效果可能很差

解决方法：

·丢弃一些特征，或者采用PCA

**·正则化，保留所有特征，减少参数大小**
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3+\theta_4x_4^4
$$
​    高次项产生过拟合，若让高次项的系数接近0，则可较好拟合。

​    修改代价函数，为系数添加惩罚项，若不知道要对哪些惩罚，可让软件自动优化选择
$$
J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]
$$
若$$\lambda$$过大，则会使所有参数都过于小，导致$$h_\theta(x)=\theta_0$$，造成欠拟合。

·正则化线性回归

![image-20240405151831314](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240405151831314.png)

·正则化逻辑回归

![image-20240405151900363](C:\Users\张丰凯\AppData\Roaming\Typora\typora-user-images\image-20240405151900363.png)

```C++
import numpy as np

def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg
```

## <u>***三、神经网络***</u>

